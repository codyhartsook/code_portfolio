{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Synopsis**:  \n",
    "This model predicts whether a given currency will break out of it's  \n",
    "current low. It is designed to aid in trend reversals and help filter  \n",
    "false breakouts. \n",
    "\n",
    "**Model Structure**:\n",
    "As currently constructed, the model contains an input layer with  \n",
    "the number of neurons equal to the number of columns in the dataset.  \n",
    "It then contains four hidden layers with ReLU activation functions  \n",
    "and an output layer with a sigmoid activation function.\n",
    "\n",
    "**Model Accuracy**:\n",
    "The model currently has a train accuracy of 0.90 and a test accuracy  \n",
    "of 0.86."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/codyhartsook/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.externals import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import sklearn as sk\n",
    "\n",
    "# keras functions to use\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import regularizers\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair</th>\n",
       "      <th>frame</th>\n",
       "      <th>rg</th>\n",
       "      <th>h</th>\n",
       "      <th>l</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>supp</th>\n",
       "      <th>o_s</th>\n",
       "      <th>p_s</th>\n",
       "      <th>obv1</th>\n",
       "      <th>obv2</th>\n",
       "      <th>obv3</th>\n",
       "      <th>rsi1</th>\n",
       "      <th>rsi2</th>\n",
       "      <th>rsi3</th>\n",
       "      <th>natr</th>\n",
       "      <th>cyc</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AUD_USD</td>\n",
       "      <td>H4</td>\n",
       "      <td>43</td>\n",
       "      <td>0.72335</td>\n",
       "      <td>0.71724</td>\n",
       "      <td>0.71832</td>\n",
       "      <td>0.72162</td>\n",
       "      <td>0.72284</td>\n",
       "      <td>0.71918</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.760388</td>\n",
       "      <td>0.912523</td>\n",
       "      <td>1.009889</td>\n",
       "      <td>32.233819</td>\n",
       "      <td>38.364744</td>\n",
       "      <td>41.044732</td>\n",
       "      <td>0.416365</td>\n",
       "      <td>2.242858</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AUD_USD</td>\n",
       "      <td>H4</td>\n",
       "      <td>44</td>\n",
       "      <td>0.72335</td>\n",
       "      <td>0.71724</td>\n",
       "      <td>0.71832</td>\n",
       "      <td>0.72162</td>\n",
       "      <td>0.72284</td>\n",
       "      <td>0.71918</td>\n",
       "      <td>-0.000061</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.760388</td>\n",
       "      <td>0.912523</td>\n",
       "      <td>1.009889</td>\n",
       "      <td>31.993635</td>\n",
       "      <td>38.123138</td>\n",
       "      <td>40.806982</td>\n",
       "      <td>0.416365</td>\n",
       "      <td>2.242858</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AUD_USD</td>\n",
       "      <td>H4</td>\n",
       "      <td>45</td>\n",
       "      <td>0.72335</td>\n",
       "      <td>0.71724</td>\n",
       "      <td>0.71832</td>\n",
       "      <td>0.72162</td>\n",
       "      <td>0.72284</td>\n",
       "      <td>0.71832</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>0.760388</td>\n",
       "      <td>0.912523</td>\n",
       "      <td>1.009889</td>\n",
       "      <td>32.010803</td>\n",
       "      <td>38.152621</td>\n",
       "      <td>40.842773</td>\n",
       "      <td>0.416365</td>\n",
       "      <td>2.242858</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AUD_USD</td>\n",
       "      <td>H4</td>\n",
       "      <td>46</td>\n",
       "      <td>0.72335</td>\n",
       "      <td>0.71724</td>\n",
       "      <td>0.71832</td>\n",
       "      <td>0.72162</td>\n",
       "      <td>0.72284</td>\n",
       "      <td>0.71832</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.004581</td>\n",
       "      <td>0.760388</td>\n",
       "      <td>0.912523</td>\n",
       "      <td>1.009889</td>\n",
       "      <td>31.490587</td>\n",
       "      <td>37.543560</td>\n",
       "      <td>40.195911</td>\n",
       "      <td>0.416365</td>\n",
       "      <td>2.242858</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AUD_USD</td>\n",
       "      <td>H4</td>\n",
       "      <td>47</td>\n",
       "      <td>0.72335</td>\n",
       "      <td>0.71724</td>\n",
       "      <td>0.71832</td>\n",
       "      <td>0.72162</td>\n",
       "      <td>0.72284</td>\n",
       "      <td>0.71832</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.760388</td>\n",
       "      <td>0.912523</td>\n",
       "      <td>1.009889</td>\n",
       "      <td>31.150450</td>\n",
       "      <td>37.181479</td>\n",
       "      <td>39.828662</td>\n",
       "      <td>0.416365</td>\n",
       "      <td>2.242858</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pair frame  rg        h        l       c1       c2       c3     supp  \\\n",
       "0  AUD_USD    H4  43  0.72335  0.71724  0.71832  0.72162  0.72284  0.71918   \n",
       "1  AUD_USD    H4  44  0.72335  0.71724  0.71832  0.72162  0.72284  0.71918   \n",
       "2  AUD_USD    H4  45  0.72335  0.71724  0.71832  0.72162  0.72284  0.71832   \n",
       "3  AUD_USD    H4  46  0.72335  0.71724  0.71832  0.72162  0.72284  0.71832   \n",
       "4  AUD_USD    H4  47  0.72335  0.71724  0.71832  0.72162  0.72284  0.71832   \n",
       "\n",
       "        o_s       p_s      obv1      obv2      obv3       rsi1       rsi2  \\\n",
       "0 -0.000094  0.000436  0.760388  0.912523  1.009889  32.233819  38.364744   \n",
       "1 -0.000061  0.001953  0.760388  0.912523  1.009889  31.993635  38.123138   \n",
       "2 -0.000029  0.003516  0.760388  0.912523  1.009889  32.010803  38.152621   \n",
       "3 -0.000003  0.004581  0.760388  0.912523  1.009889  31.490587  37.543560   \n",
       "4  0.000019  0.005376  0.760388  0.912523  1.009889  31.150450  37.181479   \n",
       "\n",
       "        rsi3      natr       cyc  result  \n",
       "0  41.044732  0.416365  2.242858     0.0  \n",
       "1  40.806982  0.416365  2.242858     0.0  \n",
       "2  40.842773  0.416365  2.242858     0.0  \n",
       "3  40.195911  0.416365  2.242858     0.0  \n",
       "4  39.828662  0.416365  2.242858     0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('breakout.csv') # read csv file of trades\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of result: 8274\n"
     ]
    }
   ],
   "source": [
    "print(\"len of result:\", len(data['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop string columns\n",
    "data.drop(data.columns[[0, 1]], axis=1, inplace=True)\n",
    "    \n",
    "# convert to float\n",
    "for col in data:\n",
    "    data[col] = pd.to_numeric(data[col])\n",
    "    \n",
    "# delete rows with nans\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 8144 entries, 0 to 8149\n",
      "Data columns (total 18 columns):\n",
      "rg        8144 non-null int64\n",
      "h         8144 non-null float64\n",
      "l         8144 non-null float64\n",
      "c1        8144 non-null float64\n",
      "c2        8144 non-null float64\n",
      "c3        8144 non-null float64\n",
      "supp      8144 non-null float64\n",
      "o_s       8144 non-null float64\n",
      "p_s       8144 non-null float64\n",
      "obv1      8144 non-null float64\n",
      "obv2      8144 non-null float64\n",
      "obv3      8144 non-null float64\n",
      "rsi1      8144 non-null float64\n",
      "rsi2      8144 non-null float64\n",
      "rsi3      8144 non-null float64\n",
      "natr      8144 non-null float64\n",
      "cyc       8144 non-null float64\n",
      "result    8144 non-null float64\n",
      "dtypes: float64(17), int64(1)\n",
      "memory usage: 1.2 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breakouts: 1879  |  false breakouts: 6265\n"
     ]
    }
   ],
   "source": [
    "pos = (list(np.where(data['result'] == 1.0)[0]))\n",
    "neg = (list(np.where(data['result'] == 0.0)[0]))\n",
    "\n",
    "print(\"breakouts:\", len(pos), \" | \", \"false breakouts:\", len(neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try to balance the results\n",
    "#pos_count = len(pos)\n",
    "#neg_count = len(neg)\n",
    "\n",
    "# we want to delete randomly\n",
    "#import random\n",
    "\n",
    "#while neg_count > pos_count:\n",
    "    \n",
    "#    rand = random.randint(1, len(neg)-1)\n",
    "#    row = neg[rand]\n",
    "#    del neg[rand]\n",
    "    \n",
    "#    data = data[data.index != row]\n",
    "#    neg_count += -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(\"breakouts:\", pos_count, \" | \", \"false breakouts:\", neg_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = range(len(data['result']))\n",
    "data['index'] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6265  |  1879\n"
     ]
    }
   ],
   "source": [
    "n = p = 0\n",
    "for row in data['result']:\n",
    "    if row < 0.4:\n",
    "        n += 1\n",
    "    else: \n",
    "        p += 1\n",
    "print(n, \" | \", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8144\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEPCAYAAABcA4N7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1051c5160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do some visualization with result column\n",
    "# we result to be evenly distributed\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = sns.lineplot(x=\"index\", \n",
    "                  y=\"result\",\n",
    "                  data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rg</th>\n",
       "      <th>h</th>\n",
       "      <th>l</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>supp</th>\n",
       "      <th>o_s</th>\n",
       "      <th>p_s</th>\n",
       "      <th>obv1</th>\n",
       "      <th>obv2</th>\n",
       "      <th>obv3</th>\n",
       "      <th>rsi1</th>\n",
       "      <th>rsi2</th>\n",
       "      <th>rsi3</th>\n",
       "      <th>natr</th>\n",
       "      <th>cyc</th>\n",
       "      <th>result</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rg</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l</th>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c1</th>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c2</th>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c3</th>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>supp</th>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o_s</th>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p_s</th>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obv1</th>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obv2</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obv3</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rsi1</th>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rsi2</th>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.73</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rsi3</th>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>natr</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.73</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cyc</th>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>result</th>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          rg     h     l    c1    c2    c3  supp   o_s   p_s  obv1  obv2  \\\n",
       "rg      1.00  0.02  0.02  0.02  0.02  0.02  0.02 -0.10 -0.20 -0.02 -0.00   \n",
       "h       0.02  1.00  1.00  1.00  1.00  1.00  1.00 -0.11 -0.02  0.05  0.05   \n",
       "l       0.02  1.00  1.00  1.00  1.00  1.00  1.00 -0.11 -0.03  0.05  0.05   \n",
       "c1      0.02  1.00  1.00  1.00  1.00  1.00  1.00 -0.11 -0.03  0.04  0.05   \n",
       "c2      0.02  1.00  1.00  1.00  1.00  1.00  1.00 -0.11 -0.02  0.05  0.05   \n",
       "c3      0.02  1.00  1.00  1.00  1.00  1.00  1.00 -0.11 -0.02  0.04  0.05   \n",
       "supp    0.02  1.00  1.00  1.00  1.00  1.00  1.00 -0.11 -0.03  0.05  0.05   \n",
       "o_s    -0.10 -0.11 -0.11 -0.11 -0.11 -0.11 -0.11  1.00  0.05 -0.08 -0.07   \n",
       "p_s    -0.20 -0.02 -0.03 -0.03 -0.02 -0.02 -0.03  0.05  1.00  0.21  0.23   \n",
       "obv1   -0.02  0.05  0.05  0.04  0.05  0.04  0.05 -0.08  0.21  1.00  0.98   \n",
       "obv2   -0.00  0.05  0.05  0.05  0.05  0.05  0.05 -0.07  0.23  0.98  1.00   \n",
       "obv3    0.02  0.03  0.03  0.03  0.03  0.03  0.03 -0.06  0.25  0.95  0.96   \n",
       "rsi1   -0.34 -0.02 -0.02 -0.02 -0.02 -0.02 -0.02  0.00  0.03  0.19  0.18   \n",
       "rsi2   -0.24  0.03  0.03  0.03  0.03  0.03  0.03  0.19  0.11  0.25  0.28   \n",
       "rsi3   -0.22 -0.02 -0.02 -0.02 -0.02 -0.02 -0.02  0.24  0.13  0.18  0.20   \n",
       "natr    0.04  0.26  0.26  0.26  0.26  0.26  0.26 -0.32  0.01  0.02  0.04   \n",
       "cyc    -0.08  0.18  0.18  0.18  0.18  0.18  0.18 -0.20 -0.06  0.01 -0.01   \n",
       "result  0.09 -0.05 -0.05 -0.05 -0.05 -0.05 -0.05  0.02  0.02 -0.01 -0.01   \n",
       "index   0.03 -0.03 -0.03 -0.03 -0.03 -0.03 -0.03  0.09 -0.02 -0.00 -0.00   \n",
       "\n",
       "        obv3  rsi1  rsi2  rsi3  natr   cyc  result  index  \n",
       "rg      0.02 -0.34 -0.24 -0.22  0.04 -0.08    0.09   0.03  \n",
       "h       0.03 -0.02  0.03 -0.02  0.26  0.18   -0.05  -0.03  \n",
       "l       0.03 -0.02  0.03 -0.02  0.26  0.18   -0.05  -0.03  \n",
       "c1      0.03 -0.02  0.03 -0.02  0.26  0.18   -0.05  -0.03  \n",
       "c2      0.03 -0.02  0.03 -0.02  0.26  0.18   -0.05  -0.03  \n",
       "c3      0.03 -0.02  0.03 -0.02  0.26  0.18   -0.05  -0.03  \n",
       "supp    0.03 -0.02  0.03 -0.02  0.26  0.18   -0.05  -0.03  \n",
       "o_s    -0.06  0.00  0.19  0.24 -0.32 -0.20    0.02   0.09  \n",
       "p_s     0.25  0.03  0.11  0.13  0.01 -0.06    0.02  -0.02  \n",
       "obv1    0.95  0.19  0.25  0.18  0.02  0.01   -0.01  -0.00  \n",
       "obv2    0.96  0.18  0.28  0.20  0.04 -0.01   -0.01  -0.00  \n",
       "obv3    1.00  0.12  0.19  0.26  0.02 -0.02    0.01  -0.01  \n",
       "rsi1    0.12  1.00  0.69  0.53  0.02 -0.02    0.02  -0.04  \n",
       "rsi2    0.19  0.69  1.00  0.73 -0.02 -0.07   -0.00  -0.00  \n",
       "rsi3    0.26  0.53  0.73  1.00 -0.03 -0.06    0.06  -0.00  \n",
       "natr    0.02  0.02 -0.02 -0.03  1.00  0.73   -0.03  -0.39  \n",
       "cyc    -0.02 -0.02 -0.07 -0.06  0.73  1.00   -0.15  -0.32  \n",
       "result  0.01  0.02 -0.00  0.06 -0.03 -0.15    1.00   0.04  \n",
       "index  -0.01 -0.04 -0.00 -0.00 -0.39 -0.32    0.04   1.00  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correlation Matrix\n",
    "corrmat = data.corr()\n",
    "corrmat.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: 369  | neg: 1260\n"
     ]
    }
   ],
   "source": [
    "X = data[['rg', 'h', 'l', 'c1', 'c2', 'c3', 'supp', 'o_s', 'p_s', 'obv1', 'obv2', 'obv3', 'rsi1', 'rsi2', 'rsi3', 'natr', 'cyc']]\n",
    "y = data['result']\n",
    "\n",
    "sc = MinMaxScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "# save sc so we can use it for predictions\n",
    "scaler_filename = \"scaler.save\"\n",
    "joblib.dump(sc, scaler_filename)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n",
    "\n",
    "neg = pos = 0\n",
    "for val in y_test:\n",
    "    if val < 0.5:\n",
    "        neg += 1\n",
    "    else:\n",
    "        pos += 1\n",
    "print(\"pos:\", pos, \" | neg:\", neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7722529158993248\n"
     ]
    }
   ],
   "source": [
    "# build a logistic regression model\n",
    "# Posted accuracy of .75 - .83 \n",
    "# accuracy not meaningful because of unbalanced nature of dataset\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(X_train, y_train)\n",
    "predictions = logisticRegr.predict(X_test)\n",
    "score = logisticRegr.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4365 samples, validate on 2150 samples\n",
      "Epoch 1/100\n",
      "4365/4365 [==============================] - 1s 279us/step - loss: 0.1831 - acc: 0.7647 - val_loss: 0.1753 - val_acc: 0.7702\n",
      "Epoch 2/100\n",
      "4365/4365 [==============================] - 1s 115us/step - loss: 0.1766 - acc: 0.7672 - val_loss: 0.1719 - val_acc: 0.7702\n",
      "Epoch 3/100\n",
      "4365/4365 [==============================] - 0s 112us/step - loss: 0.1718 - acc: 0.7672 - val_loss: 0.1660 - val_acc: 0.7702\n",
      "Epoch 4/100\n",
      "4365/4365 [==============================] - 0s 113us/step - loss: 0.1665 - acc: 0.7684 - val_loss: 0.1592 - val_acc: 0.7795\n",
      "Epoch 5/100\n",
      "4365/4365 [==============================] - 0s 112us/step - loss: 0.1623 - acc: 0.7814 - val_loss: 0.1556 - val_acc: 0.7916\n",
      "Epoch 6/100\n",
      "4365/4365 [==============================] - 1s 115us/step - loss: 0.1613 - acc: 0.7782 - val_loss: 0.1550 - val_acc: 0.7893\n",
      "Epoch 7/100\n",
      "4365/4365 [==============================] - 0s 111us/step - loss: 0.1555 - acc: 0.7904 - val_loss: 0.1556 - val_acc: 0.7898\n",
      "Epoch 8/100\n",
      "4365/4365 [==============================] - 1s 115us/step - loss: 0.1531 - acc: 0.7943 - val_loss: 0.1564 - val_acc: 0.7874\n",
      "Epoch 9/100\n",
      "4365/4365 [==============================] - 0s 112us/step - loss: 0.1500 - acc: 0.8014 - val_loss: 0.1452 - val_acc: 0.8107\n",
      "Epoch 10/100\n",
      "4365/4365 [==============================] - 0s 115us/step - loss: 0.1475 - acc: 0.8055 - val_loss: 0.1519 - val_acc: 0.7949\n",
      "Epoch 11/100\n",
      "4365/4365 [==============================] - 0s 112us/step - loss: 0.1466 - acc: 0.8014 - val_loss: 0.1472 - val_acc: 0.7995\n",
      "Epoch 12/100\n",
      "4365/4365 [==============================] - 0s 114us/step - loss: 0.1433 - acc: 0.8115 - val_loss: 0.1554 - val_acc: 0.7809\n",
      "Epoch 13/100\n",
      "4365/4365 [==============================] - 0s 112us/step - loss: 0.1403 - acc: 0.8082 - val_loss: 0.1414 - val_acc: 0.8074\n",
      "Epoch 14/100\n",
      "4365/4365 [==============================] - 1s 116us/step - loss: 0.1382 - acc: 0.8142 - val_loss: 0.1516 - val_acc: 0.7981\n",
      "Epoch 15/100\n",
      "4365/4365 [==============================] - 1s 116us/step - loss: 0.1381 - acc: 0.8094 - val_loss: 0.1396 - val_acc: 0.8140\n",
      "Epoch 16/100\n",
      "4365/4365 [==============================] - 1s 116us/step - loss: 0.1328 - acc: 0.8250 - val_loss: 0.1409 - val_acc: 0.8065\n",
      "Epoch 17/100\n",
      "4365/4365 [==============================] - 1s 116us/step - loss: 0.1301 - acc: 0.8252 - val_loss: 0.1384 - val_acc: 0.8191\n",
      "Epoch 18/100\n",
      "4365/4365 [==============================] - 1s 123us/step - loss: 0.1294 - acc: 0.8284 - val_loss: 0.1333 - val_acc: 0.8209\n",
      "Epoch 19/100\n",
      "4365/4365 [==============================] - 0s 108us/step - loss: 0.1249 - acc: 0.8309 - val_loss: 0.1434 - val_acc: 0.8065\n",
      "Epoch 20/100\n",
      "4365/4365 [==============================] - 0s 111us/step - loss: 0.1277 - acc: 0.8270 - val_loss: 0.1313 - val_acc: 0.8251\n",
      "Epoch 21/100\n",
      "4365/4365 [==============================] - 1s 118us/step - loss: 0.1221 - acc: 0.8405 - val_loss: 0.1318 - val_acc: 0.8214\n",
      "Epoch 22/100\n",
      "4365/4365 [==============================] - 1s 118us/step - loss: 0.1204 - acc: 0.8346 - val_loss: 0.1380 - val_acc: 0.8172\n",
      "Epoch 23/100\n",
      "4365/4365 [==============================] - 0s 114us/step - loss: 0.1170 - acc: 0.8477 - val_loss: 0.1252 - val_acc: 0.8367\n",
      "Epoch 24/100\n",
      "4365/4365 [==============================] - 0s 114us/step - loss: 0.1180 - acc: 0.8483 - val_loss: 0.1316 - val_acc: 0.8288\n",
      "Epoch 25/100\n",
      "4365/4365 [==============================] - 0s 114us/step - loss: 0.1110 - acc: 0.8568 - val_loss: 0.1218 - val_acc: 0.8381\n",
      "Epoch 26/100\n",
      "4365/4365 [==============================] - 1s 117us/step - loss: 0.1104 - acc: 0.8552 - val_loss: 0.1238 - val_acc: 0.8293\n",
      "Epoch 27/100\n",
      "4365/4365 [==============================] - 0s 113us/step - loss: 0.1042 - acc: 0.8625 - val_loss: 0.1169 - val_acc: 0.8409\n",
      "Epoch 28/100\n",
      "4365/4365 [==============================] - 0s 113us/step - loss: 0.1009 - acc: 0.8694 - val_loss: 0.1164 - val_acc: 0.8498\n",
      "Epoch 29/100\n",
      "4365/4365 [==============================] - 0s 114us/step - loss: 0.0995 - acc: 0.8694 - val_loss: 0.1426 - val_acc: 0.8279\n",
      "Epoch 30/100\n",
      "4365/4365 [==============================] - 1s 117us/step - loss: 0.1012 - acc: 0.8710 - val_loss: 0.1272 - val_acc: 0.8288\n",
      "Epoch 31/100\n",
      "4365/4365 [==============================] - 0s 113us/step - loss: 0.0942 - acc: 0.8779 - val_loss: 0.1149 - val_acc: 0.8549\n",
      "Epoch 32/100\n",
      "4365/4365 [==============================] - 1s 118us/step - loss: 0.0970 - acc: 0.8751 - val_loss: 0.1181 - val_acc: 0.8447\n",
      "Epoch 33/100\n",
      "4365/4365 [==============================] - 1s 116us/step - loss: 0.0923 - acc: 0.8809 - val_loss: 0.1179 - val_acc: 0.8535\n",
      "Epoch 34/100\n",
      "4365/4365 [==============================] - 1s 117us/step - loss: 0.0933 - acc: 0.8774 - val_loss: 0.1129 - val_acc: 0.8567\n",
      "Epoch 35/100\n",
      "4365/4365 [==============================] - 0s 114us/step - loss: 0.0902 - acc: 0.8841 - val_loss: 0.1138 - val_acc: 0.8549\n",
      "Epoch 36/100\n",
      "4365/4365 [==============================] - 1s 119us/step - loss: 0.0873 - acc: 0.8864 - val_loss: 0.1094 - val_acc: 0.8591\n",
      "Epoch 37/100\n",
      "4365/4365 [==============================] - 1s 115us/step - loss: 0.0834 - acc: 0.8912 - val_loss: 0.1106 - val_acc: 0.8581\n",
      "Epoch 38/100\n",
      "4365/4365 [==============================] - 0s 112us/step - loss: 0.0949 - acc: 0.8793 - val_loss: 0.1181 - val_acc: 0.8465\n",
      "Epoch 39/100\n",
      "4365/4365 [==============================] - 1s 127us/step - loss: 0.0882 - acc: 0.8882 - val_loss: 0.1059 - val_acc: 0.8674\n",
      "Epoch 40/100\n",
      "4365/4365 [==============================] - 1s 115us/step - loss: 0.0897 - acc: 0.8843 - val_loss: 0.1138 - val_acc: 0.8526\n",
      "Epoch 41/100\n",
      "4365/4365 [==============================] - 0s 108us/step - loss: 0.0885 - acc: 0.8877 - val_loss: 0.1053 - val_acc: 0.8679\n",
      "Epoch 42/100\n",
      "4365/4365 [==============================] - 0s 113us/step - loss: 0.0783 - acc: 0.9006 - val_loss: 0.1160 - val_acc: 0.8493\n",
      "Epoch 43/100\n",
      "4365/4365 [==============================] - 0s 114us/step - loss: 0.0797 - acc: 0.8978 - val_loss: 0.1073 - val_acc: 0.8605\n",
      "Epoch 44/100\n",
      "4365/4365 [==============================] - 0s 113us/step - loss: 0.0807 - acc: 0.8981 - val_loss: 0.1063 - val_acc: 0.8679\n",
      "Epoch 45/100\n",
      "4365/4365 [==============================] - 0s 114us/step - loss: 0.0821 - acc: 0.8951 - val_loss: 0.1143 - val_acc: 0.8600\n",
      "Epoch 46/100\n",
      "4365/4365 [==============================] - 1s 115us/step - loss: 0.0781 - acc: 0.9017 - val_loss: 0.1146 - val_acc: 0.8642\n",
      "Epoch 47/100\n",
      "4365/4365 [==============================] - 0s 114us/step - loss: 0.0802 - acc: 0.9015 - val_loss: 0.1117 - val_acc: 0.8581\n",
      "Epoch 48/100\n",
      "4365/4365 [==============================] - ETA: 0s - loss: 0.0807 - acc: 0.898 - 1s 117us/step - loss: 0.0789 - acc: 0.9008 - val_loss: 0.1147 - val_acc: 0.8586\n",
      "Epoch 49/100\n",
      "4365/4365 [==============================] - 1s 115us/step - loss: 0.0794 - acc: 0.8983 - val_loss: 0.0990 - val_acc: 0.8795\n",
      "Epoch 50/100\n",
      "4365/4365 [==============================] - 0s 113us/step - loss: 0.0753 - acc: 0.9054 - val_loss: 0.1047 - val_acc: 0.8712\n",
      "Epoch 51/100\n",
      "4365/4365 [==============================] - 1s 116us/step - loss: 0.0735 - acc: 0.9061 - val_loss: 0.1348 - val_acc: 0.8121\n",
      "Epoch 52/100\n",
      "4365/4365 [==============================] - 1s 117us/step - loss: 0.0810 - acc: 0.8942 - val_loss: 0.1009 - val_acc: 0.8805\n",
      "Epoch 53/100\n",
      "4365/4365 [==============================] - 0s 107us/step - loss: 0.0707 - acc: 0.9125 - val_loss: 0.1074 - val_acc: 0.8707\n",
      "Epoch 54/100\n",
      "4365/4365 [==============================] - 1s 115us/step - loss: 0.0728 - acc: 0.9084 - val_loss: 0.1050 - val_acc: 0.8740\n",
      "Epoch 55/100\n",
      "4365/4365 [==============================] - 1s 126us/step - loss: 0.0722 - acc: 0.9090 - val_loss: 0.0976 - val_acc: 0.8809\n",
      "Epoch 56/100\n",
      "4365/4365 [==============================] - 0s 113us/step - loss: 0.0716 - acc: 0.9093 - val_loss: 0.1026 - val_acc: 0.8721\n",
      "Epoch 57/100\n",
      "4365/4365 [==============================] - 0s 111us/step - loss: 0.0665 - acc: 0.9173 - val_loss: 0.1009 - val_acc: 0.8781\n",
      "Epoch 58/100\n",
      "4365/4365 [==============================] - 0s 111us/step - loss: 0.0745 - acc: 0.9040 - val_loss: 0.1114 - val_acc: 0.8591\n",
      "Epoch 59/100\n",
      "4365/4365 [==============================] - 0s 114us/step - loss: 0.0716 - acc: 0.9116 - val_loss: 0.1058 - val_acc: 0.8684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100\n",
      "4365/4365 [==============================] - 0s 109us/step - loss: 0.0681 - acc: 0.9162 - val_loss: 0.1089 - val_acc: 0.8735\n",
      "Epoch 61/100\n",
      "4365/4365 [==============================] - 0s 112us/step - loss: 0.0723 - acc: 0.9090 - val_loss: 0.0974 - val_acc: 0.8828\n",
      "Epoch 62/100\n",
      "4365/4365 [==============================] - 0s 112us/step - loss: 0.0642 - acc: 0.9191 - val_loss: 0.0964 - val_acc: 0.8851\n",
      "Epoch 63/100\n",
      "4365/4365 [==============================] - 0s 110us/step - loss: 0.0637 - acc: 0.9191 - val_loss: 0.0977 - val_acc: 0.8795\n",
      "Epoch 64/100\n",
      "4365/4365 [==============================] - 0s 113us/step - loss: 0.0642 - acc: 0.9203 - val_loss: 0.0976 - val_acc: 0.8842\n",
      "Epoch 65/100\n",
      "4365/4365 [==============================] - 0s 112us/step - loss: 0.0612 - acc: 0.9253 - val_loss: 0.0968 - val_acc: 0.8828\n",
      "Epoch 66/100\n",
      "4365/4365 [==============================] - 0s 111us/step - loss: 0.0660 - acc: 0.9175 - val_loss: 0.1038 - val_acc: 0.8749\n",
      "Epoch 67/100\n",
      "4365/4365 [==============================] - 0s 107us/step - loss: 0.0644 - acc: 0.9216 - val_loss: 0.1038 - val_acc: 0.8740\n",
      "Epoch 68/100\n",
      "4365/4365 [==============================] - 0s 113us/step - loss: 0.0616 - acc: 0.9246 - val_loss: 0.0997 - val_acc: 0.8786\n",
      "Epoch 69/100\n",
      "4365/4365 [==============================] - 0s 113us/step - loss: 0.0698 - acc: 0.9136 - val_loss: 0.1030 - val_acc: 0.8753\n",
      "Epoch 70/100\n",
      "4365/4365 [==============================] - 0s 113us/step - loss: 0.0614 - acc: 0.9239 - val_loss: 0.0943 - val_acc: 0.8870\n",
      "Epoch 71/100\n",
      "2200/4365 [==============>...............] - ETA: 0s - loss: 0.0543 - acc: 0.9359"
     ]
    }
   ],
   "source": [
    "# Build the deep neural net: Posted Accuracy of .9 on train, .85 on test\n",
    "\n",
    "# Number of neurons in the first layer = number of columns in dataset\n",
    "# Activation function = ReLU\n",
    "# 5 Hidden layers for a total of 6 layers in the network\n",
    "# Last layer activation function = Sigmoid\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', \n",
    "                                                  np.unique(y_train), \n",
    "                                                  y_train)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# input layer\n",
    "model.add(Dense(100, activation='relu', input_dim = 17))\n",
    "          \n",
    "# second layer\n",
    "model.add(Dense(60, activation='relu'))\n",
    "          \n",
    "# third layer\n",
    "model.add(Dense(60, activation='relu'))\n",
    "          \n",
    "# fourth layer\n",
    "model.add(Dense(60, activation='relu'))\n",
    "\n",
    "# fith layer\n",
    "model.add(Dense(60, activation='relu'))\n",
    "          \n",
    "# output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model using rmsprop opt and mse loss\n",
    "model.compile(optimizer = 'adam',     \n",
    "              loss = 'mse',\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_split = 0.33, \n",
    "                    epochs = 100,\n",
    "                    #class_weight = class_weights,\n",
    "                    batch_size = 25)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Training set accuracy:', train_acc)\n",
    "print('Test set accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The history of our accuracy during training.\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The history of our cross-entropy loss during training.\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "neg = pos = 0\n",
    "for pred in predictions:\n",
    "    if pred < 0.5:\n",
    "        neg += 1\n",
    "    else:\n",
    "        pos += 1\n",
    "print(\"pos:\", pos, \" | neg:\", neg)\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model for later use\n",
    "\n",
    "from keras.models import load_model\n",
    "model.save('DNN_Breakout')  # creates a HDF5 file\n",
    "del model  # deletes the existing model\n",
    "\n",
    "# load model to make sure that it was correctly saved\n",
    "model = load_model('DNN_Breakout')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test set accuracy:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
